{"cells":[{"cell_type":"markdown","source":["# Ensuring Consistency with ACID Transactions with Delta Lake (Loan Risk Data)\n\n<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-logo-whitebackground.png\" width=200/>\n\nThis is a companion notebook to provide a Delta Lake example against the Lending Club data.\n* This notebook has been tested with *DBR 5.4 ML Beta, Python 3*\n* For the bottom sections of this notebook, you will need `mlflow` and `yellowbrick` installed on your cluster as well"],"metadata":{}},{"cell_type":"markdown","source":["## The Data\n\nThe data used is public data from Lending Club. It includes all funded loans from 2012 to 2017. Each loan includes applicant information provided by the applicant as well as the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. For a full view of the data please view the data dictionary available [here](https://resources.lendingclub.com/LCDataDictionary.xlsx).\n\n\n![Loan_Data](https://preview.ibb.co/d3tQ4R/Screen_Shot_2018_02_02_at_11_21_51_PM.png)\n\nhttps://www.kaggle.com/wendykan/lending-club-loan-data"],"metadata":{}},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Delta Lake\n\nOptimization Layer a top blob storage for Reliability (i.e. ACID compliance) and Low Latency of Streaming + Batch data pipelines."],"metadata":{}},{"cell_type":"markdown","source":["## Import Data and create pre-Delta Lake Table\n* This will create a lot of small Parquet files emulating the typical small file problem that occurs with streaming or highly transactional data"],"metadata":{}},{"cell_type":"code","source":["# -----------------------------------------------\n# Uncomment and run if this folder does not exist\n# -----------------------------------------------\n# Configure location of loanstats_2012_2017.parquet\nlspq_path = \"/databricks-datasets/samples/lending_club/parquet/\"\n\n# Read loanstats_2012_2017.parquet\ndata = spark.read.parquet(lspq_path)\n\n# Reduce the amount of data (to run on DBCE)\n(loan_stats, loan_stats_rest) = data.randomSplit([0.01, 0.99], seed=123)\n\n# Select only the columns needed\nloan_stats = loan_stats.select(\"addr_state\", \"loan_status\")\n\n# Create loan by state\nloan_by_state = loan_stats.groupBy(\"addr_state\").count()\n\n# Create table\nloan_by_state.createOrReplaceTempView(\"loan_by_state\")\n\n# Display loans by state\ndisplay(loan_by_state)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>count</th></tr></thead><tbody><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>OR</td><td>169</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>CA</td><td>1934</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>NE</td><td>38</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>WA</td><td>322</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>HI</td><td>71</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Easily Convert Parquet to Delta Lake format\nWith Delta Lake, you can easily transform your Parquet data into Delta Lake format."],"metadata":{}},{"cell_type":"code","source":["# Configure Delta Lake Silver Path\nDELTALAKE_SILVER_PATH = \"/ml/loan_by_state_delta\"\n\n# Remove folder if it exists\ndbutils.fs.rm(DELTALAKE_SILVER_PATH, recurse=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: True</div>"]}}],"execution_count":7},{"cell_type":"code","source":["%sql \n-- Current example is creating a new table instead of in-place import so will need to change this code\nDROP TABLE IF EXISTS loan_by_state_delta;\n\nCREATE TABLE loan_by_state_delta\nUSING delta\nLOCATION '/ml/loan_by_state_delta'\nAS SELECT * FROM loan_by_state;\n\n-- View Delta Lake table\nSELECT * FROM loan_by_state_delta"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>count</th></tr></thead><tbody><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>OR</td><td>169</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>CA</td><td>1934</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>HI</td><td>71</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>WA</td><td>322</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>NE</td><td>38</td></tr></tbody></table></div>"]}}],"execution_count":8},{"cell_type":"code","source":["%sql \nDESCRIBE DETAIL delta.`/ml/loan_by_state_delta`"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th></tr></thead><tbody><tr><td>delta</td><td>71944f78-30ad-4af4-8c68-60a4febca488</td><td>null</td><td>null</td><td>dbfs:/ml/loan_by_state_delta</td><td>2020-03-27T02:45:58.445+0000</td><td>2020-03-27T02:46:42.000+0000</td><td>List()</td><td>46</td><td>30501</td><td>Map()</td><td>1</td><td>2</td></tr></tbody></table></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## Stop the notebook before the streaming cell, in case of a \"run all\""],"metadata":{}},{"cell_type":"code","source":["dbutils.notebook.exit(\"stop\") "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/plain":["stop"]}}],"execution_count":11},{"cell_type":"code","source":["%fs ls /ml/loan_by_state_delta/_delta_log/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/.s3-optimization-0</td><td>.s3-optimization-0</td><td>0</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/.s3-optimization-1</td><td>.s3-optimization-1</td><td>0</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/.s3-optimization-2</td><td>.s3-optimization-2</td><td>0</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000000.crc</td><td>00000000000000000000.crc</td><td>91</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000000.json</td><td>00000000000000000000.json</td><td>16790</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Unified Batch and Streaming Source and Sink\n\nThese cells showcase streaming and batch concurrent queries (inserts and reads)\n* This notebook will run an `INSERT` every 10s against our `loan_stats_delta` table\n* We will run two streaming queries concurrently against this data\n* Note, you can also use `writeStream` but this version is easier to run in DBCE"],"metadata":{}},{"cell_type":"code","source":["# Read the insertion of data\nloan_by_state_readStream = spark.readStream.format(\"delta\").load(DELTALAKE_SILVER_PATH)\nloan_by_state_readStream.createOrReplaceTempView(\"loan_by_state_readStream\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["%sql\nselect addr_state, sum(`count`) as loans from loan_by_state_readStream group by addr_state"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>loans</th></tr></thead><tbody><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>OR</td><td>169</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>CA</td><td>1934</td></tr><tr><td>NE</td><td>38</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>WA</td><td>322</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>IA</td><td>2700</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>HI</td><td>71</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["**Wait** until the stream is up and running before executing the code below"],"metadata":{}},{"cell_type":"code","source":["import time\ni = 1\nwhile i <= 6:\n  # Execute Insert statement\n  insert_sql = \"INSERT INTO loan_by_state_delta VALUES ('IA', 450)\"\n  spark.sql(insert_sql)\n  print('loan_by_state_delta: inserted new row of data, loop: [%s]' % i)\n    \n  # Loop through\n  i = i + 1\n  time.sleep(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">loan_by_state_delta: inserted new row of data, loop: [1]\nloan_by_state_delta: inserted new row of data, loop: [2]\nloan_by_state_delta: inserted new row of data, loop: [3]\nloan_by_state_delta: inserted new row of data, loop: [4]\nloan_by_state_delta: inserted new row of data, loop: [5]\nloan_by_state_delta: inserted new row of data, loop: [6]\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["%fs ls /ml/loan_by_state_delta/_delta_log/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/.s3-optimization-0</td><td>.s3-optimization-0</td><td>0</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/.s3-optimization-1</td><td>.s3-optimization-1</td><td>0</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/.s3-optimization-2</td><td>.s3-optimization-2</td><td>0</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000000.crc</td><td>00000000000000000000.crc</td><td>91</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000000.json</td><td>00000000000000000000.json</td><td>16790</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000001.crc</td><td>00000000000000000001.crc</td><td>91</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000001.json</td><td>00000000000000000001.json</td><td>773</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000002.crc</td><td>00000000000000000002.crc</td><td>91</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000002.json</td><td>00000000000000000002.json</td><td>773</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000003.crc</td><td>00000000000000000003.crc</td><td>91</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000003.json</td><td>00000000000000000003.json</td><td>773</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000004.crc</td><td>00000000000000000004.crc</td><td>91</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000004.json</td><td>00000000000000000004.json</td><td>773</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000005.crc</td><td>00000000000000000005.crc</td><td>91</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000005.json</td><td>00000000000000000005.json</td><td>773</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000006.crc</td><td>00000000000000000006.crc</td><td>91</td></tr><tr><td>dbfs:/ml/loan_by_state_delta/_delta_log/00000000000000000006.json</td><td>00000000000000000006.json</td><td>773</td></tr></tbody></table></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["**Note**: Once the previous cell is finished and the state of Iowa is fully populated in the map (in cell 14), click *Cancel* in Cell 14 to stop the `readStream`."],"metadata":{}},{"cell_type":"markdown","source":["Let's review our current set of loans using our map visualization."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Review current loans within the `loan_by_state_delta` Delta Lake table\nselect addr_state, sum(`count`) as loans from loan_by_state_delta group by addr_state"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>loans</th></tr></thead><tbody><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>OR</td><td>169</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>CA</td><td>1934</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>NE</td><td>38</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>WA</td><td>322</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>IA</td><td>2700</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>HI</td><td>71</td></tr></tbody></table></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["Observe that the Iowa (middle state) has the largest number of loans due to the recent stream of data.  Note that the original `loan_by_state_delta` table is updated as we're reading `loan_by_state_readStream`."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["##![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Full DML Support\n\n**Note**: Full DML Support is a feature that will be coming soon to Delta Lake; the preview is currently available in Databricks.\n\nDelta Lake supports standard DML including UPDATE, DELETE and MERGE INTO providing developers more controls to manage their big datasets."],"metadata":{}},{"cell_type":"markdown","source":["Let's start by creating a traditional Parquet table"],"metadata":{}},{"cell_type":"code","source":["# Load new DataFrame based on current Delta table\nlbs_df = sql(\"select * from loan_by_state_delta\")\n\n# Save DataFrame to Parquet\nlbs_df.write.mode(\"overwrite\").parquet(\"loan_by_state.parquet\")\n\n# Reload Parquet Data\nlbs_pq = spark.read.parquet(\"loan_by_state.parquet\")\n\n# Create new table on this parquet data\nlbs_pq.createOrReplaceTempView(\"loan_by_state_pq\")\n\n# Review data\ndisplay(sql(\"select * from loan_by_state_pq\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>count</th></tr></thead><tbody><tr><td>IA</td><td>450</td></tr><tr><td>IA</td><td>450</td></tr><tr><td>IA</td><td>450</td></tr><tr><td>IA</td><td>450</td></tr><tr><td>IA</td><td>450</td></tr><tr><td>IA</td><td>450</td></tr><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>OR</td><td>169</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>CA</td><td>1934</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>HI</td><td>71</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>WA</td><td>322</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>NE</td><td>38</td></tr></tbody></table></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["###![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) DELETE Support\n\nThe data was originally supposed to be assigned to `WA` state, so let's `DELETE` those values assigned to `IA`"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Attempting to run `DELETE` on the Parquet table\nDELETE FROM loan_by_state_pq WHERE addr_state = 'IA'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: DELETE destination only supports Delta sources.\nSome(Relation[addr_state#6048,count#6049L] parquet\n);\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.notADeltaSourceException(DeltaErrors.scala:225)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableDelete$$anonfun$apply0$1.applyOrElse(PreprocessTableDelete.scala:50)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableDelete$$anonfun$apply0$1.applyOrElse(PreprocessTableDelete.scala:44)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableDelete.apply0(PreprocessTableDelete.scala:44)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableDelete.apply(PreprocessTableDelete.scala:40)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableDelete.apply(PreprocessTableDelete.scala:34)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:834)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$2.apply(Dataset.scala:90)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$2.apply(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:834)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sql$1.apply(SparkSession.scala:692)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sql$1.apply(SparkSession.scala:687)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:834)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:687)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:126)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":28},{"cell_type":"markdown","source":["**Note**: This command fails because the `DELETE` statements are not supported in Parquet, but are supported in Delta Lake."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Running `DELETE` on the Delta Lake table\nDELETE FROM loan_by_state_delta WHERE addr_state = 'IA'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"code","source":["%sql\n-- Review current loans within the `loan_by_state_delta` Delta Lake table\nselect addr_state, sum(`count`) as loans from loan_by_state_delta group by addr_state"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>loans</th></tr></thead><tbody><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>OR</td><td>169</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>CA</td><td>1934</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>NE</td><td>38</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>WA</td><td>322</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>HI</td><td>71</td></tr></tbody></table></div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["###![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) UPDATE Support\nThe data was originally supposed to be assigned to `WA` state, so let's `UPDATE` those values"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Attempting to run `UPDATE` on the Parquet table\nUPDATE loan_by_state_pq SET `count` = 2700 WHERE addr_state = 'WA'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: UPDATE destination only supports Delta sources.\nSome(Relation[addr_state#6048,count#6049L] parquet\n);\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.notADeltaSourceException(DeltaErrors.scala:225)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableUpdateEdge$$anonfun$apply0$1.applyOrElse(PreprocessTableUpdateEdge.scala:56)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableUpdateEdge$$anonfun$apply0$1.applyOrElse(PreprocessTableUpdateEdge.scala:50)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableUpdateEdge.apply0(PreprocessTableUpdateEdge.scala:50)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableUpdateEdge.apply(PreprocessTableUpdateEdge.scala:46)\n\tat com.databricks.sql.transaction.tahoe.PreprocessTableUpdateEdge.apply(PreprocessTableUpdateEdge.scala:39)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:834)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$2.apply(Dataset.scala:90)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$2.apply(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:834)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sql$1.apply(SparkSession.scala:692)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sql$1.apply(SparkSession.scala:687)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:834)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:687)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:126)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":33},{"cell_type":"markdown","source":["**Note**: This command fails because the `UPDATE` statements are not supported in Parquet, but are supported in Delta Lake."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Running `UPDATE` on the Delta Lake table\nUPDATE loan_by_state_delta SET `count` = 2700 WHERE addr_state = 'WA'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":35},{"cell_type":"code","source":["%sql\n-- Review current loans within the `loan_by_state_delta` Delta Lake table\nselect addr_state, sum(`count`) as loans from loan_by_state_delta group by addr_state"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>loans</th></tr></thead><tbody><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>OR</td><td>169</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>CA</td><td>1934</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>NE</td><td>38</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>WA</td><td>2700</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>HI</td><td>71</td></tr></tbody></table></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["###![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) MERGE INTO Support\n\n#### INSERT or UPDATE parquet: 7-step process\n\nWith a legacy data pipeline, to insert or update a table, you must:\n1. Identify the new rows to be inserted\n2. Identify the rows that will be replaced (i.e. updated)\n3. Identify all of the rows that are not impacted by the insert or update\n4. Create a new temp based on all three insert statements\n5. Delete the original table (and all of those associated files)\n6. \"Rename\" the temp table back to the original table name\n7. Drop the temp table\n\n![](https://pages.databricks.com/rs/094-YMS-629/images/merge-into-legacy.gif)\n\n\n#### INSERT or UPDATE with Delta Lake\n\n2-step process: \n1. Identify rows to insert or update\n2. Use `MERGE`"],"metadata":{}},{"cell_type":"code","source":["# Let's create a simple table to merge\nitems = [('IA', 10), ('CA', 2500), ('OR', None)]\ncols = ['addr_state', 'count']\nmerge_table = spark.createDataFrame(items, cols)\nmerge_table.createOrReplaceTempView(\"merge_table\")\ndisplay(merge_table)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>count</th></tr></thead><tbody><tr><td>IA</td><td>10</td></tr><tr><td>CA</td><td>2500</td></tr><tr><td>OR</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["Instead of writing separate `INSERT` and `UPDATE` statements, we can use a `MERGE` statement."],"metadata":{}},{"cell_type":"code","source":["%sql\nMERGE INTO loan_by_state_delta as d\nUSING merge_table as m\non d.addr_state = m.addr_state\nWHEN MATCHED THEN \n  UPDATE SET *\nWHEN NOT MATCHED \n  THEN INSERT *"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":40},{"cell_type":"code","source":["%sql\n-- Review current loans within the `loan_by_state_delta` Delta Lake table\nselect addr_state, sum(`count`) as loans from loan_by_state_delta group by addr_state"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>loans</th></tr></thead><tbody><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>OR</td><td>null</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>CA</td><td>2500</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>NE</td><td>38</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>WA</td><td>2700</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>IA</td><td>10</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>HI</td><td>71</td></tr></tbody></table></div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["##![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Schema Evolution\nWith the `mergeSchema` option, you can evolve your Delta Lake table schema"],"metadata":{}},{"cell_type":"code","source":["# Generate new loans with dollar amounts \nloans = sql(\"select addr_state, cast(rand(10)*count as bigint) as count, cast(rand(10) * 10000 * count as double) as amount from loan_by_state_delta\")\ndisplay(loans)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>count</th><th>amount</th></tr></thead><tbody><tr><td>MN</td><td>110</td><td>1100475.641577956</td></tr><tr><td>NJ</td><td>399</td><td>3999510.4471974433</td></tr><tr><td>VA</td><td>384</td><td>3847504.692080366</td></tr><tr><td>RI</td><td>4</td><td>48094.0460786387</td></tr><tr><td>MI</td><td>146</td><td>1468441.3520793389</td></tr><tr><td>NV</td><td>123</td><td>1236372.6420862195</td></tr><tr><td>WI</td><td>96</td><td>965623.6328830952</td></tr><tr><td>ID</td><td>2</td><td>29073.325486820133</td></tr><tr><td>MT</td><td>29</td><td>296729.5016017777</td></tr><tr><td>NC</td><td>283</td><td>2838029.0661800858</td></tr><tr><td>VT</td><td>10</td><td>104884.23144904031</td></tr><tr><td>MD</td><td>227</td><td>2278586.408747159</td></tr><tr><td>DE</td><td>23</td><td>231037.05923644523</td></tr><tr><td>ME</td><td>10</td><td>108616.64647589102</td></tr><tr><td>MS</td><td>27</td><td>272716.60521870933</td></tr><tr><td>AL</td><td>46</td><td>467302.79746270867</td></tr><tr><td>IN</td><td>132</td><td>1322894.3638420864</td></tr><tr><td>OH</td><td>33</td><td>339129.7158019902</td></tr><tr><td>TN</td><td>41</td><td>411841.2118856987</td></tr><tr><td>NM</td><td>24</td><td>247107.33860622777</td></tr><tr><td>PA</td><td>215</td><td>2156347.9228779557</td></tr><tr><td>SD</td><td>17</td><td>177094.5547510617</td></tr><tr><td>NY</td><td>986</td><td>9864344.670032812</td></tr><tr><td>TX</td><td>772</td><td>7724727.373799967</td></tr><tr><td>WV</td><td>6</td><td>64620.17597710523</td></tr><tr><td>GA</td><td>92</td><td>928006.3906815298</td></tr><tr><td>MA</td><td>44</td><td>444996.3408047334</td></tr><tr><td>KS</td><td>94</td><td>942935.4510224281</td></tr><tr><td>AR</td><td>101</td><td>1010193.7392476556</td></tr><tr><td>HI</td><td>22</td><td>223124.38110475725</td></tr><tr><td>LA</td><td>32</td><td>327351.6803623572</td></tr><tr><td>DC</td><td>2</td><td>25918.882578660785</td></tr><tr><td>NH</td><td>8</td><td>81379.42715133085</td></tr><tr><td>CA</td><td>536</td><td>5361345.589824823</td></tr><tr><td>MO</td><td>235</td><td>2355355.0022845245</td></tr><tr><td>IL</td><td>492</td><td>4926076.692687503</td></tr><tr><td>IA</td><td>9</td><td>90484.95847713486</td></tr><tr><td>AK</td><td>31</td><td>318523.2664263007</td></tr><tr><td>OK</td><td>82</td><td>822303.0742826762</td></tr><tr><td>UT</td><td>31</td><td>314033.67170657136</td></tr><tr><td>WA</td><td>1047</td><td>1.047361002267291E7</td></tr><tr><td>ND</td><td>11</td><td>116738.8069241638</td></tr><tr><td>WY</td><td>12</td><td>121140.83372306294</td></tr><tr><td>KY</td><td>78</td><td>788534.1837007209</td></tr><tr><td>FL</td><td>137</td><td>1377240.8016581847</td></tr><tr><td>CO</td><td>0</td><td>8005.7279294378695</td></tr><tr><td>AZ</td><td>320</td><td>3206270.441291474</td></tr><tr><td>SC</td><td>144</td><td>1444805.508009956</td></tr><tr><td>CT</td><td>212</td><td>2123864.1341696503</td></tr><tr><td>NE</td><td>7</td><td>73636.57857107832</td></tr><tr><td>OR</td><td>null</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":43},{"cell_type":"code","source":["# Let's write this data out to our Delta table\nloans.write.format(\"delta\").mode(\"append\").save(DELTALAKE_SILVER_PATH)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o570.save.\n: org.apache.spark.sql.AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 160060ee-8700-454f-b52a-60fa2cfac4b9).\nTo enable schema migration, please set:\n&#39;.option(&#34;mergeSchema&#34;, &#34;true&#34;)&#39;.\n\nTable schema:\nroot\n-- addr_state: string (nullable = true)\n-- count: long (nullable = true)\n\n\nData schema:\nroot\n-- addr_state: string (nullable = true)\n-- count: long (nullable = true)\n-- amount: double (nullable = true)\n\n         \nIf Table ACLs are enabled, these options will be ignored. Please use the ALTER TABLE\ncommand for changing the schema.\n        ;\n\tat com.databricks.sql.transaction.tahoe.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:948)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation$class.updateMetadata(ImplicitMetadataOperation.scala:125)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.updateMetadata(WriteIntoDelta.scala:50)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:91)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:71)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:207)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:825)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:128)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:191)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:834)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:306)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:292)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-330632782662291&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># Let&#39;s write this data out to our Delta table</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>loans<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;delta&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;append&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>DELTALAKE_SILVER_PATH<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    737</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    738</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 739</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    740</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    741</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;A schema mismatch detected when writing to the Delta table (Table ID: 160060ee-8700-454f-b52a-60fa2cfac4b9).\\nTo enable schema migration, please set:\\n\\&#39;.option(&#34;mergeSchema&#34;, &#34;true&#34;)\\&#39;.\\n\\nTable schema:\\nroot\\n-- addr_state: string (nullable = true)\\n-- count: long (nullable = true)\\n\\n\\nData schema:\\nroot\\n-- addr_state: string (nullable = true)\\n-- count: long (nullable = true)\\n-- amount: double (nullable = true)\\n\\n         \\nIf Table ACLs are enabled, these options will be ignored. Please use the ALTER TABLE\\ncommand for changing the schema.\\n        ;&#39;</div>"]}}],"execution_count":44},{"cell_type":"markdown","source":["**Note**: This command fails because the schema of our new data does not match the schema of our original data"],"metadata":{}},{"cell_type":"code","source":["# Add the mergeSchema option\nloans.write.option(\"mergeSchema\",\"true\").format(\"delta\").mode(\"append\").save(DELTALAKE_SILVER_PATH)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":46},{"cell_type":"markdown","source":["**Note**: With the `mergeSchema` option, we can merge these different schemas together."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Review current loans within the `loan_by_state_delta` Delta Lake table\nselect addr_state, sum(`amount`) as amount from loan_by_state_delta group by addr_state order by sum(`amount`) desc limit 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>amount</th></tr></thead><tbody><tr><td>WA</td><td>1.047361002267291E7</td></tr><tr><td>NY</td><td>9864344.670032812</td></tr><tr><td>TX</td><td>7724727.373799967</td></tr><tr><td>CA</td><td>5361345.589824823</td></tr><tr><td>IL</td><td>4926076.692687503</td></tr><tr><td>NJ</td><td>3999510.4471974433</td></tr><tr><td>VA</td><td>3847504.692080366</td></tr><tr><td>AZ</td><td>3206270.441291474</td></tr><tr><td>NC</td><td>2838029.0661800858</td></tr><tr><td>MO</td><td>2355355.0022845245</td></tr></tbody></table></div>"]}}],"execution_count":48},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Let's Travel back in Time!\nDatabricks Delta’s time travel capabilities simplify building data pipelines for the following use cases. \n\n* Audit Data Changes\n* Reproduce experiments & reports\n* Rollbacks\n\nAs you write into a Delta table or directory, every operation is automatically versioned.\n\nYou can query by:\n1. Using a timestamp\n1. Using a version number\n\nusing Python, Scala, and/or Scala syntax; for these examples we will use the SQL syntax.  \n\nFor more information, refer to [Introducing Delta Time Travel for Large Scale Data Lakes](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)"],"metadata":{}},{"cell_type":"markdown","source":["### ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Review Delta Lake Table History\nAll the transactions for this table are stored within this table including the initial set of insertions, update, delete, merge, and inserts with schema modification"],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY loan_by_state_delta"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th></tr></thead><tbody><tr><td>10</td><td>2020-03-26T22:10:31.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>9</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 2, numOutputBytes -> 2669, numOutputRows -> 51, numParts -> 0)</td></tr><tr><td>9</td><td>2020-03-26T22:10:00.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>MERGE</td><td>Map(predicate -> (d.`addr_state` = m.`addr_state`))</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>8</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numFiles -> 4, numTargetFilesAfterSkipping -> 47, numTargetFilesAdded -> 4, numTargetRowsInserted -> 1, numTargetRowsUpdated -> 2, numOutputRows -> 3, numParts -> 0, numOutputBytes -> 2290, numSourceRows -> 3, numTargetFilesRemoved -> 2, numTargetFilesBeforeSkipping -> 47)</td></tr><tr><td>8</td><td>2020-03-26T22:09:29.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>UPDATE</td><td>Map(predicate -> (addr_state#6451 = WA))</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>7</td><td>WriteSerializable</td><td>false</td><td>Map(numTotalRows -> 4, numFiles -> 1, numRemovedFiles -> 1, numCopiedRows -> 3, numOutputRows -> 2, numParts -> 0, numOutputBytes -> 667, numAddedFiles -> 1, numUpdatedRows -> 1)</td></tr><tr><td>7</td><td>2020-03-26T22:09:12.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>DELETE</td><td>Map(predicate -> [\"(default.loan_by_state_delta.`addr_state` = 'IA')\"])</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>6</td><td>WriteSerializable</td><td>false</td><td>Map(numTotalRows -> 10, numFiles -> 1, numRemovedFiles -> 9, numCopiedRows -> 4, numDeletedRows -> 6, numOutputRows -> 0, numParts -> 0, numOutputBytes -> 372, numAddedFiles -> 1)</td></tr><tr><td>6</td><td>2020-03-26T22:07:32.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>5</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputBytes -> 669, numOutputRows -> 1, numParts -> 0)</td></tr><tr><td>5</td><td>2020-03-26T22:07:22.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>4</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputBytes -> 669, numOutputRows -> 1, numParts -> 0)</td></tr><tr><td>4</td><td>2020-03-26T22:07:10.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>3</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputBytes -> 669, numOutputRows -> 1, numParts -> 0)</td></tr><tr><td>3</td><td>2020-03-26T22:06:56.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>2</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputBytes -> 669, numOutputRows -> 1, numParts -> 0)</td></tr><tr><td>2</td><td>2020-03-26T22:06:43.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>1</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputBytes -> 669, numOutputRows -> 1, numParts -> 0)</td></tr><tr><td>1</td><td>2020-03-26T22:06:30.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputBytes -> 669, numOutputRows -> 1, numParts -> 0)</td></tr><tr><td>0</td><td>2020-03-26T22:05:57.000+0000</td><td>2015887990336516</td><td>npabb001@odu.edu</td><td>CREATE TABLE AS SELECT</td><td>Map(isManaged -> false, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(330632782662247)</td><td>0326-215822-rite948</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 46, numOutputBytes -> 30500, numOutputRows -> 50, numParts -> 0)</td></tr></tbody></table></div>"]}}],"execution_count":51},{"cell_type":"markdown","source":["### ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Time Travel via Version Number\nBelow are SQL syntax examples of Delta Time Travel by using a Version Number"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM loan_by_state_delta VERSION AS OF 0"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>count</th></tr></thead><tbody><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>OR</td><td>169</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>CA</td><td>1934</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>HI</td><td>71</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>WA</td><td>322</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>NE</td><td>38</td></tr></tbody></table></div>"]}}],"execution_count":53},{"cell_type":"code","source":["%sql\nSELECT * FROM loan_by_state_delta VERSION AS OF 9"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>count</th></tr></thead><tbody><tr><td>MN</td><td>266</td></tr><tr><td>NJ</td><td>547</td></tr><tr><td>VA</td><td>426</td></tr><tr><td>RI</td><td>51</td></tr><tr><td>MI</td><td>383</td></tr><tr><td>NV</td><td>222</td></tr><tr><td>WI</td><td>194</td></tr><tr><td>ID</td><td>14</td></tr><tr><td>MT</td><td>31</td></tr><tr><td>NC</td><td>382</td></tr><tr><td>VT</td><td>31</td></tr><tr><td>MD</td><td>340</td></tr><tr><td>DE</td><td>36</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>MS</td><td>80</td></tr><tr><td>AL</td><td>188</td></tr><tr><td>IN</td><td>248</td></tr><tr><td>OH</td><td>526</td></tr><tr><td>TN</td><td>212</td></tr><tr><td>NM</td><td>94</td></tr><tr><td>PA</td><td>518</td></tr><tr><td>SD</td><td>36</td></tr><tr><td>NY</td><td>1280</td></tr><tr><td>TX</td><td>1254</td></tr><tr><td>WV</td><td>49</td></tr><tr><td>GA</td><td>468</td></tr><tr><td>MA</td><td>350</td></tr><tr><td>KS</td><td>124</td></tr><tr><td>AR</td><td>121</td></tr><tr><td>HI</td><td>71</td></tr><tr><td>LA</td><td>165</td></tr><tr><td>DC</td><td>30</td></tr><tr><td>NH</td><td>66</td></tr><tr><td>CA</td><td>2500</td></tr><tr><td>MO</td><td>239</td></tr><tr><td>IL</td><td>608</td></tr><tr><td>IA</td><td>10</td></tr><tr><td>AK</td><td>42</td></tr><tr><td>OK</td><td>134</td></tr><tr><td>UT</td><td>86</td></tr><tr><td>WA</td><td>2700</td></tr><tr><td>ND</td><td>21</td></tr><tr><td>WY</td><td>28</td></tr><tr><td>KY</td><td>151</td></tr><tr><td>FL</td><td>1019</td></tr><tr><td>CO</td><td>316</td></tr><tr><td>AZ</td><td>346</td></tr><tr><td>SC</td><td>188</td></tr><tr><td>CT</td><td>235</td></tr><tr><td>NE</td><td>38</td></tr><tr><td>OR</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":54},{"cell_type":"markdown","source":["## Run Our Model\nLet's run a simple linear regression model to predict the number of loans based on the population of the state\n* The following shell statements downloads the us_census_2020 data that we will join with the `loan_by_state_delta` table"],"metadata":{}},{"cell_type":"code","source":["%sh mkdir -p /dbfs/tmp/sais_eu_19_demo/census/ && wget -O /dbfs/tmp/sais_eu_19_demo/census/us_census_2010.csv https://pages.databricks.com/rs/094-YMS-629/images/us_census_2010.csv && ls -al /dbfs/tmp/sais_eu_19_demo/census/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">--2020-03-26 22:11:11--  https://pages.databricks.com/rs/094-YMS-629/images/us_census_2010.csv\nResolving pages.databricks.com (pages.databricks.com)... 104.17.72.206, 104.17.73.206, 104.17.74.206, ...\nConnecting to pages.databricks.com (pages.databricks.com)|104.17.72.206|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 591 [text/plain]\nSaving to: ‘/dbfs/tmp/sais_eu_19_demo/census/us_census_2010.csv’\n\n     0K                                                       100% 70.5M=0s\n\n2020-03-26 22:11:12 (70.5 MB/s) - ‘/dbfs/tmp/sais_eu_19_demo/census/us_census_2010.csv’ saved [591/591]\n\ntotal 9\ndrwxrwxrwx 2 root root 4096 Mar 26  2020 .\ndrwxrwxrwx 2 root root 4096 Mar 26 22:02 ..\n-rwxrwxrwx 1 root root  591 Mar 26  2020 us_census_2010.csv\n</div>"]}}],"execution_count":56},{"cell_type":"markdown","source":["### Notes\nIf you forgot to install `mlflow` and `yellowbrick` on your cluster, instead of re-running everything all over again:\n* Note that the Delta Lake table is stored in `DELTALAKE_SILVERPATH` or `/ml/loan_by_state_delta`\n* You can add the libraries, restart the cluster and then start reading the data from the following cells (instead of rerunning everything all over again)\n* Just uncomment the cell below to reconnect to your Delta Table"],"metadata":{}},{"cell_type":"code","source":["## Recreate loan_by_state_delta view\n# spark.read.format(\"delta\").load(\"/ml/loan_by_state_delta\").createOrReplaceTempView(\"loan_by_state_delta\")\n## Check data\n# display(spark.sql(\"select count(1) from loan_by_state_delta\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":58},{"cell_type":"code","source":["# Include census data\ncensus = spark.read.csv('/tmp/sais_eu_19_demo/census/us_census_2010.csv', sep=',', inferSchema=True, header=True)\ncensus.createOrReplaceTempView(\"census\")\n\n# Data versions (0, 6, 9)\ndfv0 = spark.sql(\"select c.Population, l.count as label from (select addr_state as State, count from loan_by_state_delta  version as of 0) l left outer join census c on c.State = l.State\")\ndfv6 = spark.sql(\"select c.Population, l.count as label from (select addr_state as State, count from loan_by_state_delta  version as of 6) l left outer join census c on c.State = l.State\")\ndfv9 = spark.sql(\"select c.Population, l.count as label from (select addr_state as State, count from loan_by_state_delta  version as of 9 where count is not null) l left outer join census c on c.State = l.State\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":59},{"cell_type":"code","source":["# Import MLflow\nimport mlflow\nimport mlflow.spark\n#print(\"MLflow Version: %s\" % mlflow.__version__)\n\n# Display Residuals (yellowbrick)\ndef displayResiduals(train, test):\n  from sklearn.linear_model import Ridge\n  from yellowbrick.regressor import ResidualsPlot\n\n  # define feature columns\n  featureColumns = ['Population']\n\n  # Create pandas DataFrames\n  pdf_train = train.toPandas()\n  pdf_test = test.toPandas()\n\n  # Convert to X, y train and test values\n  X_train = pdf_train[['Population']].to_numpy()\n  y_train = pdf_train[['label']].to_numpy().flatten()\n  X_test = pdf_test[['Population']].to_numpy()\n  y_test = pdf_test[['label']].to_numpy().flatten()  \n  \n  # Instantiate the linear model and visualizer\n  ridge = Ridge()\n  visualizer = ResidualsPlot(ridge)\n\n  # Visualize\n  visualizer.fit(X_train, y_train)  # Fit the training data to the model\n  visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n  visualizer.show(outpath=\"ridge-model-residuals.png\")\n  #fig=visualizer.poof()             # Draw/show/poof the data\n  \n# Predict Loan Count (based on state population)\ndef predictLoanCount(df, data_version):\n  from pyspark.ml.linalg import Vectors\n  from pyspark.ml.feature import VectorAssembler\n  from pyspark.ml.regression import LinearRegression\n  from pyspark.ml.evaluation import RegressionEvaluator\n  \n  # assemble vector\n  assembler = VectorAssembler(\n    inputCols=[\"Population\"],\n    outputCol=\"features\")\n  output = assembler.transform(df)\n\n  # Log mlflow\n  with mlflow.start_run() as run:  \n    # Define LinearRegression algorithm\n    lr = LinearRegression(maxIter=10, regParam=0.0, elasticNetParam=0.8)\n    model = lr.fit(output)  \n\n    # Calculate predictions\n    predictions = model.transform(output)\n\n    # calculate RMSE\n    evaluator = RegressionEvaluator(metricName=\"rmse\")\n    RMSE = evaluator.evaluate(predictions)\n    #print(\"Model: Root Mean Squared Error = \" + str(RMSE))\n\n    # Log Parameters\n    mlflow.log_param(\"data version\", data_version)\n    mlflow.log_metric(\"RMSE\", RMSE)\n\n    # Log Model\n    mlflow.spark.log_model(model, \"model\") \n    \n    # Save if not data_version is \"v0\"\n    if (data_version != 'v0'):\n      # Log artifacts (output files)\n      mlflow.log_artifact(\"ridge-model-residuals.png\")\n  \n  # return predictions DataFrame\n  #return predictions"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":60},{"cell_type":"code","source":["# Calculate predictions\n# Initial version of data (v0)\npredictLoanCount(dfv0, 'v0')\n\n# Version 6 (after streaming of Iowa data)\ndisplayResiduals(dfv0, dfv6)\npredictLoanCount(dfv6, 'v6')\n\n# Version 9 (after correction of data: update, delete, merge)\ndisplayResiduals(dfv0, dfv9)\npredictLoanCount(dfv9, 'v9')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-330632782662308&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-red-fg\"># Version 6 (after streaming of Iowa data)</span>\n<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\"> </span>displayResiduals<span class=\"ansi-blue-fg\">(</span>dfv0<span class=\"ansi-blue-fg\">,</span> dfv6<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> predictLoanCount<span class=\"ansi-blue-fg\">(</span>dfv6<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;v6&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> \n\n<span class=\"ansi-green-fg\">&lt;command-330632782662307&gt;</span> in <span class=\"ansi-cyan-fg\">displayResiduals</span><span class=\"ansi-blue-fg\">(train, test)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> <span class=\"ansi-green-fg\">def</span> displayResiduals<span class=\"ansi-blue-fg\">(</span>train<span class=\"ansi-blue-fg\">,</span> test<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>   <span class=\"ansi-green-fg\">from</span> sklearn<span class=\"ansi-blue-fg\">.</span>linear_model <span class=\"ansi-green-fg\">import</span> Ridge\n<span class=\"ansi-green-fg\">----&gt; 9</span><span class=\"ansi-red-fg\">   </span><span class=\"ansi-green-fg\">from</span> yellowbrick<span class=\"ansi-blue-fg\">.</span>regressor <span class=\"ansi-green-fg\">import</span> ResidualsPlot\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span>   <span class=\"ansi-red-fg\"># define feature columns</span>\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;yellowbrick&#39;</div>"]}}],"execution_count":61},{"cell_type":"markdown","source":["#### Review RMSE and Residuals\n* Review the RMSE values via the MLflow Sidebar"],"metadata":{}}],"metadata":{"name":"01a-Delta Lake Workshop - Delta Lake Primer + MLflow","notebookId":330632782662247},"nbformat":4,"nbformat_minor":0}
